## thinking
当一个输入可以有多种正确输出时，使用单一目标的交叉熵损失和贪心解码会导致以下问题：

- 损失函数不准确：模型可能生成了一个合理的描述，但因为它与训练集中的特定参考不匹配而受到惩罚
- 训练不稳定：相同输入对应的不同参考可能互相矛盾，导致模型无法收敛
- 评估不公平：贪心解码只能生成一种输出，无法体现模型可能具备的多样性


由于验证集中一条mr可以对应多条 ref：
- val_loss 描述不准确，故去除
- 使用 bleu4 作为验证集指标，但是仍然要解决多条 ref 的问题：模型输入mr得到一个输出，该输出与多条ref计算bleu4，取最高的数值



## explore

关于梯度裁剪clip：
- 梯度裁剪 clip 设置较小的值可以让模型快速收敛，但是后期波动大，可能说明收敛的方向不对（局部最优）
- 梯度裁剪设置较大值或者不设限，那么模型收敛会变慢，并且前期梯度大，更有可能跳出局部极值
- 不进行梯度裁剪可能导致梯度爆炸，不过本次任务是短文本分类，故影响不大

关于teacher forcing：
- 训练时完全使用 teacher forcing 效果还不错
- 但是容易过拟合
- 本次任务如果使用 scheduling， 反倒效果不好（为什么？数据集太小了？）

关于交叉熵损失函数：
- torch的类提供了接口，可以忽略掉 pad 标签的相关损失，可是似乎造成了模型没能正确的认识到eos的含义？

关于掩码注意力和lstm处理填充的batch序列
- 事先准备一个 valid_src_len 是十分有必要的

